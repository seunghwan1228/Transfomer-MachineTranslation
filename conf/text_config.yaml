# Path
preprocess_text_path: prepare_data/text

# Dataset_name
dataset_name: 'ted_hrlr_translate/pt_to_en'

# Tokenizer
# Tokenizer Options
# 'subword', 'sentencepiece', 'word
tokenizer: 'sentencepiece'


vocab_size: 32000

# Sentence-Piece Config
lang_one_file: 'lang_one.txt'
lang_two_file: 'lang_two.txt'
pad_id: 0
bos_id: 1
eos_id: 2
unk_id: 3
spm_model_saver: 'prepare_data/spm/'
model_prefix: 'prepare_data/spm/tp_spm'
character_coverage: 1.0

# SentencePiece Model Type
# Model Type options
# 'bpe', 'unigram', 'char', 'word'
model_type: 'bpe'


# Dataset
batch_size: 32

# Model
max_pos_length: 10000
num_heads: 8
model_dim: 512
feed_forward_dim: 1024
dropout_rate: 0.2
mha_concat_query: True
n_layers: 6


# Model Debug Mode
debug: False