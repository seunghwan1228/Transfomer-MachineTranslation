# Path
preprocess_text_path: prepare_data/text

# Dataset_name
dataset_name: 'ted_hrlr_translate/pt_to_en'

# Tokenizer
# Tokenizer Options
# 'subword', 'sentencepiece', 'word
tokenizer: 'word'



# Sentence-Piece Config
vocab_size: 32000
lang_one_file: 'lang_one.txt'
lang_two_file: 'lang_two.txt'
pad_id: 0
bos_id: 1
eos_id: 2
unk_id: 3
spm_model_saver: 'prepare_data/spm/'
model_prefix: 'prepare_data/spm/tp_spm'
character_coverage: 1.0
# SentencePiece Model Type
# Model Type options
# 'bpe', 'unigram', 'char', 'word'
model_type: 'bpe'


# Dataset
batch_size: 32

# Model
max_pos_length: 10000
num_heads: 8
model_dim: 512
feed_forward_dim: 1024
dropout_rate: 0.2
mha_concat_query: True
n_layers: 6

# Model Training
log_dir: './model_log/train'
max_to_keep: 5
keep_n_hours: 1

epoch: 100
drop_n_heads: 2


# Model Debug Mode
debug: False